Reverts gcc/libgcc/unwind-dw2-fde.(c|h) to version from GCC 12.

This is temporary workaround for: 

https://gcc.gnu.org/bugzilla/show_bug.cgi?id=110955

--- gcc-13.4.0/libgcc/unwind-dw2-fde.c
+++ gcc-13.4.0/libgcc/unwind-dw2-fde.c
@@ -42,40 +42,14 @@ #define ATOMIC_FDE_FAST_PATH 1
 #endif
 #endif
 
-typedef __UINTPTR_TYPE__ uintptr_type;
-
-#ifdef ATOMIC_FDE_FAST_PATH
-#include "unwind-dw2-btree.h"
-
-static struct btree registered_frames;
-static struct btree registered_objects;
-static bool in_shutdown;
-
-static void
-release_registered_frames (void) __attribute__ ((destructor));
-static void
-release_registered_frames (void)
-{
-  /* Release the b-tree and all frames. Frame releases that happen later are
-   * silently ignored */
-  btree_destroy (&registered_frames);
-  btree_destroy (&registered_objects);
-  in_shutdown = true;
-}
-
-static void
-get_pc_range (const struct object *ob, uintptr_type *range);
-
-#else
-/* Without fast path frame deregistration must always succeed.  */
-static const int in_shutdown = 0;
-
 /* The unseen_objects list contains objects that have been registered
    but not yet categorized in any way.  The seen_objects list has had
    its pc_begin and count fields initialized at minimum, and is sorted
    by decreasing value of pc_begin.  */
 static struct object *unseen_objects;
 static struct object *seen_objects;
+#ifdef ATOMIC_FDE_FAST_PATH
+static int any_objects_registered;
 #endif
 
 #ifdef __GTHREAD_MUTEX_INIT
@@ -105,21 +79,6 @@ static __gthread_mutex_t object_mutex;
 #endif
 #endif
 
-#ifdef ATOMIC_FDE_FAST_PATH
-// Register the pc range for a given object in the lookup structure.
-static void
-register_pc_range_for_object (uintptr_type begin, struct object *ob)
-{
-  // Register the object itself to know the base pointer on deregistration.
-  btree_insert (&registered_objects, begin, 1, ob);
-
-  // Register the frame in the b-tree
-  uintptr_type range[2];
-  get_pc_range (ob, range);
-  btree_insert (&registered_frames, range[0], range[1] - range[0], ob);
-}
-#endif
-
 /* Called from crtbegin.o to register the unwind info for an object.  */
 
 void
@@ -140,17 +99,23 @@ #ifdef DWARF2_OBJECT_END_PTR_EXTENSION
   ob->fde_end = NULL;
 #endif
 
-#ifdef ATOMIC_FDE_FAST_PATH
-  register_pc_range_for_object ((uintptr_type) begin, ob);
-#else
   init_object_mutex_once ();
   __gthread_mutex_lock (&object_mutex);
 
   ob->next = unseen_objects;
   unseen_objects = ob;
+#ifdef ATOMIC_FDE_FAST_PATH
+  /* Set flag that at least one library has registered FDEs.
+     Use relaxed MO here, it is up to the app to ensure that the library
+     loading/initialization happens-before using that library in other
+     threads (in particular unwinding with that library's functions
+     appearing in the backtraces).  Calling that library's functions
+     without waiting for the library to initialize would be racy.  */
+  if (!any_objects_registered)
+    __atomic_store_n (&any_objects_registered, 1, __ATOMIC_RELAXED);
+#endif
 
   __gthread_mutex_unlock (&object_mutex);
-#endif
 }
 
 void
@@ -188,17 +153,23 @@ ob->s.i = 0;
   ob->s.b.from_array = 1;
   ob->s.b.encoding = DW_EH_PE_omit;
 
-#ifdef ATOMIC_FDE_FAST_PATH
-  register_pc_range_for_object ((uintptr_type) begin, ob);
-#else
   init_object_mutex_once ();
   __gthread_mutex_lock (&object_mutex);
 
   ob->next = unseen_objects;
   unseen_objects = ob;
+#ifdef ATOMIC_FDE_FAST_PATH
+  /* Set flag that at least one library has registered FDEs.
+     Use relaxed MO here, it is up to the app to ensure that the library
+     loading/initialization happens-before using that library in other
+     threads (in particular unwinding with that library's functions
+     appearing in the backtraces).  Calling that library's functions
+     without waiting for the library to initialize would be racy.  */
+  if (!any_objects_registered)
+    __atomic_store_n (&any_objects_registered, 1, __ATOMIC_RELAXED);
+#endif
 
   __gthread_mutex_unlock (&object_mutex);
-#endif
 }
 
 void
@@ -229,35 +200,16 @@ implements __register_frame_info_bases.
 void *
 __deregister_frame_info_bases (const void *begin)
 {
+  struct object **p;
   struct object *ob = 0;
 
   /* If .eh_frame is empty, we haven't registered.  */
   if ((const uword *) begin == 0 || *(const uword *) begin == 0)
     return ob;
 
-#ifdef ATOMIC_FDE_FAST_PATH
-  // Find the originally registered object to get the base pointer.
-  ob = btree_remove (&registered_objects, (uintptr_type) begin);
-
-  // Remove the corresponding PC range.
-  if (ob)
-    {
-      uintptr_type range[2];
-      get_pc_range (ob, range);
-      if (range[0] != range[1])
-	btree_remove (&registered_frames, range[0]);
-    }
-
-  // Deallocate the sort array if any.
-  if (ob && ob->s.b.sorted)
-    {
-      free (ob->u.sort);
-    }
-#else
   init_object_mutex_once ();
   __gthread_mutex_lock (&object_mutex);
 
-  struct object **p;
   for (p = &unseen_objects; *p ; p = &(*p)->next)
     if ((*p)->u.single == begin)
       {
@@ -289,11 +241,7 @@ }
 
  out:
   __gthread_mutex_unlock (&object_mutex);
-#endif
-
-  // If we didn't find anything in the lookup data structures then they
-  // were either already destroyed or we tried to remove an empty range.
-  gcc_assert (in_shutdown || ob);
+  gcc_assert (ob);
   return (void *) ob;
 }
 
@@ -316,7 +264,7 @@ /* Like base_of_encoded_value, but take
    instead of an _Unwind_Context.  */
 
 static _Unwind_Ptr
-base_from_object (unsigned char encoding, const struct object *ob)
+base_from_object (unsigned char encoding, struct object *ob)
 {
   if (encoding == DW_EH_PE_omit)
     return 0;
@@ -463,52 +411,21 @@ }
 
 typedef int (*fde_compare_t) (struct object *, const fde *, const fde *);
 
-// The extractor functions compute the pointer values for a block of
-// fdes. The block processing hides the call overhead.
-
-static void
-fde_unencoded_extract (struct object *ob __attribute__ ((unused)),
-		       _Unwind_Ptr *target, const fde **x, int count)
-{
-  for (int index = 0; index < count; ++index)
-    memcpy (target + index, x[index]->pc_begin, sizeof (_Unwind_Ptr));
-}
-
-static void
-fde_single_encoding_extract (struct object *ob, _Unwind_Ptr *target,
-			     const fde **x, int count)
-{
-  _Unwind_Ptr base;
-
-  base = base_from_object (ob->s.b.encoding, ob);
-  for (int index = 0; index < count; ++index)
-    read_encoded_value_with_base (ob->s.b.encoding, base, x[index]->pc_begin,
-				  target + index);
-}
-
-static void
-fde_mixed_encoding_extract (struct object *ob, _Unwind_Ptr *target,
-			    const fde **x, int count)
-{
-  for (int index = 0; index < count; ++index)
-    {
-      int encoding = get_fde_encoding (x[index]);
-      read_encoded_value_with_base (encoding, base_from_object (encoding, ob),
-				    x[index]->pc_begin, target + index);
-    }
-}
-
-typedef void (*fde_extractor_t) (struct object *, _Unwind_Ptr *, const fde **,
-				 int);
-
-// Data is is sorted using radix sort if possible, using an temporary
-// auxiliary data structure of the same size as the input. When running
-// out of memory do in-place heap sort.
+/* This is a special mix of insertion sort and heap sort, optimized for
+   the data sets that actually occur. They look like
+   101 102 103 127 128 105 108 110 190 111 115 119 125 160 126 129 130.
+   I.e. a linearly increasing sequence (coming from functions in the text
+   section), with additionally a few unordered elements (coming from functions
+   in gnu_linkonce sections) whose values are higher than the values in the
+   surrounding linear sequence (but not necessarily higher than the values
+   at the end of the linear sequence!).
+   The worst-case total run time is O(N) + O(n log (n)), where N is the
+   total number of FDEs and n is the number of erratic ones.  */
 
 struct fde_accumulator
 {
   struct fde_vector *linear;
-  struct fde_vector *aux;
+  struct fde_vector *erratic;
 };
 
 static inline int
@@ -522,8 +439,8 @@ size = sizeof (struct fde_vector) + size
   if ((accu->linear = malloc (size)))
     {
       accu->linear->count = 0;
-      if ((accu->aux = malloc (size)))
-	accu->aux->count = 0;
+      if ((accu->erratic = malloc (size)))
+	accu->erratic->count = 0;
       return 1;
     }
   else
@@ -537,6 +454,59 @@ if (accu->linear)
     accu->linear->array[accu->linear->count++] = this_fde;
 }
 
+/* Split LINEAR into a linear sequence with low values and an erratic
+   sequence with high values, put the linear one (of longest possible
+   length) into LINEAR and the erratic one into ERRATIC. This is O(N).
+
+   Because the longest linear sequence we are trying to locate within the
+   incoming LINEAR array can be interspersed with (high valued) erratic
+   entries.  We construct a chain indicating the sequenced entries.
+   To avoid having to allocate this chain, we overlay it onto the space of
+   the ERRATIC array during construction.  A final pass iterates over the
+   chain to determine what should be placed in the ERRATIC array, and
+   what is the linear sequence.  This overlay is safe from aliasing.  */
+
+static inline void
+fde_split (struct object *ob, fde_compare_t fde_compare,
+	   struct fde_vector *linear, struct fde_vector *erratic)
+{
+  static const fde *marker;
+  size_t count = linear->count;
+  const fde *const *chain_end = &marker;
+  size_t i, j, k;
+
+  /* This should optimize out, but it is wise to make sure this assumption
+     is correct. Should these have different sizes, we cannot cast between
+     them and the overlaying onto ERRATIC will not work.  */
+  gcc_assert (sizeof (const fde *) == sizeof (const fde **));
+
+  for (i = 0; i < count; i++)
+    {
+      const fde *const *probe;
+
+      for (probe = chain_end;
+	   probe != &marker && fde_compare (ob, linear->array[i], *probe) < 0;
+	   probe = chain_end)
+	{
+	  chain_end = (const fde *const*) erratic->array[probe - linear->array];
+	  erratic->array[probe - linear->array] = NULL;
+	}
+      erratic->array[i] = (const fde *) chain_end;
+      chain_end = &linear->array[i];
+    }
+
+  /* Each entry in LINEAR which is part of the linear sequence we have
+     discovered will correspond to a non-NULL entry in the chain we built in
+     the ERRATIC array.  */
+  for (i = j = k = 0; i < count; i++)
+    if (erratic->array[i])
+      linear->array[j++] = linear->array[i];
+    else
+      erratic->array[k++] = linear->array[i];
+  linear->count = j;
+  erratic->count = k;
+}
+
 #define SWAP(x,y) do { const fde * tmp = x; x = y; y = tmp; } while (0)
 
 /* Convert a semi-heap to a heap.  A semi-heap is a heap except possibly
@@ -599,130 +569,71 @@ }
 #undef SWAP
 }
 
-// Radix sort data in V1 using V2 as aux memory. Runtime O(n).
+/* Merge V1 and V2, both sorted, and put the result into V1.  */
 static inline void
-fde_radixsort (struct object *ob, fde_extractor_t fde_extractor,
-	       struct fde_vector *v1, struct fde_vector *v2)
+fde_merge (struct object *ob, fde_compare_t fde_compare,
+	   struct fde_vector *v1, struct fde_vector *v2)
 {
-#define FANOUTBITS 8
-#define FANOUT (1 << FANOUTBITS)
-#define BLOCKSIZE 128
-  const unsigned rounds
-    = (__CHAR_BIT__ * sizeof (_Unwind_Ptr) + FANOUTBITS - 1) / FANOUTBITS;
-  const fde **a1 = v1->array, **a2 = v2->array;
-  _Unwind_Ptr ptrs[BLOCKSIZE + 1];
-  unsigned n = v1->count;
-  for (unsigned round = 0; round != rounds; ++round)
-    {
-      unsigned counts[FANOUT] = {0};
-      unsigned violations = 0;
-
-      // Count the number of elements per bucket and check if we are already
-      // sorted.
-      _Unwind_Ptr last = 0;
-      for (unsigned i = 0; i < n;)
-	{
-	  unsigned chunk = ((n - i) <= BLOCKSIZE) ? (n - i) : BLOCKSIZE;
-	  fde_extractor (ob, ptrs + 1, a1 + i, chunk);
-	  ptrs[0] = last;
-	  for (unsigned j = 0; j < chunk; ++j)
-	    {
-	      unsigned b = (ptrs[j + 1] >> (round * FANOUTBITS)) & (FANOUT - 1);
-	      counts[b]++;
-	      // Use summation instead of an if to eliminate branches.
-	      violations += ptrs[j + 1] < ptrs[j];
-	    }
-	  i += chunk;
-	  last = ptrs[chunk];
-	}
-
-      // Stop if we are already sorted.
-      if (!violations)
-	{
-	  break;
-	}
+  size_t i1, i2;
+  const fde * fde2;
 
-      // Compute the prefix sum.
-      unsigned sum = 0;
-      for (unsigned i = 0; i != FANOUT; ++i)
-	{
-	  unsigned s = sum;
-	  sum += counts[i];
-	  counts[i] = s;
-	}
-
-      // Place all elements.
-      for (unsigned i = 0; i < n;)
+  i2 = v2->count;
+  if (i2 > 0)
+    {
+      i1 = v1->count;
+      do
 	{
-	  unsigned chunk = ((n - i) <= BLOCKSIZE) ? (n - i) : BLOCKSIZE;
-	  fde_extractor (ob, ptrs, a1 + i, chunk);
-	  for (unsigned j = 0; j < chunk; ++j)
+	  i2--;
+	  fde2 = v2->array[i2];
+	  while (i1 > 0 && fde_compare (ob, v1->array[i1-1], fde2) > 0)
 	    {
-	      unsigned b = (ptrs[j] >> (round * FANOUTBITS)) & (FANOUT - 1);
-	      a2[counts[b]++] = a1[i + j];
+	      v1->array[i1+i2] = v1->array[i1-1];
+	      i1--;
 	    }
-	  i += chunk;
+	  v1->array[i1+i2] = fde2;
 	}
-
-      // Swap a1 and a2.
-      const fde **tmp = a1;
-      a1 = a2;
-      a2 = tmp;
-    }
-#undef BLOCKSIZE
-#undef FANOUT
-#undef FANOUTBITS
-
-  // The data is in a1 now, move in place if needed.
-  if (a1 != v1->array)
-    memcpy (v1->array, a1, sizeof (const fde *) * n);
+      while (i2 > 0);
+      v1->count += v2->count;
+    }
 }
 
 static inline void
 end_fde_sort (struct object *ob, struct fde_accumulator *accu, size_t count)
 {
+  fde_compare_t fde_compare;
+
   gcc_assert (!accu->linear || accu->linear->count == count);
 
-  if (accu->aux)
-    {
-      fde_extractor_t fde_extractor;
-      if (ob->s.b.mixed_encoding)
-	fde_extractor = fde_mixed_encoding_extract;
-      else if (ob->s.b.encoding == DW_EH_PE_absptr)
-	fde_extractor = fde_unencoded_extract;
-      else
-	fde_extractor = fde_single_encoding_extract;
+  if (ob->s.b.mixed_encoding)
+    fde_compare = fde_mixed_encoding_compare;
+  else if (ob->s.b.encoding == DW_EH_PE_absptr)
+    fde_compare = fde_unencoded_compare;
+  else
+    fde_compare = fde_single_encoding_compare;
 
-      fde_radixsort (ob, fde_extractor, accu->linear, accu->aux);
-      free (accu->aux);
+  if (accu->erratic)
+    {
+      fde_split (ob, fde_compare, accu->linear, accu->erratic);
+      gcc_assert (accu->linear->count + accu->erratic->count == count);
+      frame_heapsort (ob, fde_compare, accu->erratic);
+      fde_merge (ob, fde_compare, accu->linear, accu->erratic);
+      free (accu->erratic);
     }
   else
     {
-      fde_compare_t fde_compare;
-      if (ob->s.b.mixed_encoding)
-	fde_compare = fde_mixed_encoding_compare;
-      else if (ob->s.b.encoding == DW_EH_PE_absptr)
-	fde_compare = fde_unencoded_compare;
-      else
-	fde_compare = fde_single_encoding_compare;
-
-      /* We've not managed to malloc an aux array,
+      /* We've not managed to malloc an erratic array,
 	 so heap sort in the linear one.  */
       frame_heapsort (ob, fde_compare, accu->linear);
     }
 }
 
-/* Inspect the fde array beginning at this_fde. This
-   function can be used either in query mode (RANGE is
-   not null, OB is const), or in update mode (RANGE is
-   null, OB is modified). In query mode the function computes
-   the range of PC values and stores it in RANGE. In
-   update mode it updates encoding, mixed_encoding, and pc_begin
-   for OB. Return the number of fdes encountered along the way. */
+
+/* Update encoding, mixed_encoding, and pc_begin for OB for the
+   fde array beginning at THIS_FDE.  Return the number of fdes
+   encountered along the way.  */
 
 static size_t
-classify_object_over_fdes (struct object *ob, const fde *this_fde,
-			   uintptr_type *range)
+classify_object_over_fdes (struct object *ob, const fde *this_fde)
 {
   const struct dwarf_cie *last_cie = 0;
   size_t count = 0;
@@ -748,18 +659,14 @@ encoding = get_cie_encoding (this_cie);
 	  if (encoding == DW_EH_PE_omit)
 	    return -1;
 	  base = base_from_object (encoding, ob);
-	  if (!range)
-	    {
-	      if (ob->s.b.encoding == DW_EH_PE_omit)
-		ob->s.b.encoding = encoding;
-	      else if (ob->s.b.encoding != encoding)
-		ob->s.b.mixed_encoding = 1;
-	    }
+	  if (ob->s.b.encoding == DW_EH_PE_omit)
+	    ob->s.b.encoding = encoding;
+	  else if (ob->s.b.encoding != encoding)
+	    ob->s.b.mixed_encoding = 1;
 	}
 
-      const unsigned char *p;
-      p = read_encoded_value_with_base (encoding, base, this_fde->pc_begin,
-					&pc_begin);
+      read_encoded_value_with_base (encoding, base, this_fde->pc_begin,
+				    &pc_begin);
 
       /* Take care to ignore link-once functions that were removed.
 	 In these cases, the function address will be NULL, but if
@@ -775,29 +682,8 @@ if ((pc_begin & mask) == 0)
 	continue;
 
       count += 1;
-      if (range)
-	{
-	  _Unwind_Ptr pc_range, pc_end;
-	  read_encoded_value_with_base (encoding & 0x0F, 0, p, &pc_range);
-	  pc_end = pc_begin + pc_range;
-	  if ((!range[0]) && (!range[1]))
-	    {
-	      range[0] = pc_begin;
-	      range[1] = pc_end;
-	    }
-	  else
-	    {
-	      if (pc_begin < range[0])
-		range[0] = pc_begin;
-	      if (pc_end > range[1])
-		range[1] = pc_end;
-	    }
-	}
-      else
-	{
-	  if ((void *) pc_begin < ob->pc_begin)
-	    ob->pc_begin = (void *) pc_begin;
-	}
+      if ((void *) pc_begin < ob->pc_begin)
+	ob->pc_begin = (void *) pc_begin;
     }
 
   return count;
@@ -882,7 +768,7 @@ {
 	  fde **p = ob->u.array;
 	  for (count = 0; *p; ++p)
 	    {
-	      size_t cur_count = classify_object_over_fdes (ob, *p, NULL);
+	      size_t cur_count = classify_object_over_fdes (ob, *p);
 	      if (cur_count == (size_t) -1)
 		goto unhandled_fdes;
 	      count += cur_count;
@@ -890,7 +776,7 @@ }
 	}
       else
 	{
-	  count = classify_object_over_fdes (ob, ob->u.single, NULL);
+	  count = classify_object_over_fdes (ob, ob->u.single);
 	  if (count == (size_t) -1)
 	    {
 	      static const fde terminator;
@@ -931,42 +817,8 @@ DSO will deregister the object.  */
   accu.linear->orig_data = ob->u.single;
   ob->u.sort = accu.linear;
 
-#ifdef ATOMIC_FDE_FAST_PATH
-  // We must update the sorted bit with an atomic operation
-  struct object tmp;
-  tmp.s.b = ob->s.b;
-  tmp.s.b.sorted = 1;
-  __atomic_store (&(ob->s.b), &(tmp.s.b), __ATOMIC_RELEASE);
-#else
   ob->s.b.sorted = 1;
-#endif
-}
-
-#ifdef ATOMIC_FDE_FAST_PATH
-/* Get the PC range for lookup */
-static void
-get_pc_range (const struct object *ob, uintptr_type *range)
-{
-  // It is safe to cast to non-const object* here as
-  // classify_object_over_fdes does not modify ob in query mode.
-  struct object *ncob = (struct object *) (uintptr_type) ob;
-  range[0] = range[1] = 0;
-  if (ob->s.b.sorted)
-    {
-      classify_object_over_fdes (ncob, ob->u.sort->orig_data, range);
-    }
-  else if (ob->s.b.from_array)
-    {
-      fde **p = ob->u.array;
-      for (; *p; ++p)
-	classify_object_over_fdes (ncob, *p, range);
-    }
-  else
-    {
-      classify_object_over_fdes (ncob, ob->u.single, range);
-    }
 }
-#endif
 
 /* A linear search through a set of FDEs for the given PC.  This is
    used when there was insufficient memory to allocate and sort an
@@ -1132,9 +984,6 @@ }
 static const fde *
 search_object (struct object* ob, void *pc)
 {
-  /* The fast path initializes objects eagerly to avoid locking.
-   * On the slow path we initialize them now */
-#ifndef ATOMIC_FDE_FAST_PATH
   /* If the data hasn't been sorted, try to do this now.  We may have
      more memory available than last time we tried.  */
   if (! ob->s.b.sorted)
@@ -1147,7 +996,6 @@ check is in order.  */
       if (pc < ob->pc_begin)
 	return NULL;
     }
-#endif
 
   if (ob->s.b.sorted)
     {
@@ -1177,21 +1025,6 @@ return linear_search_fdes (ob, ob->u.sin
     }
 }
 
-#ifdef ATOMIC_FDE_FAST_PATH
-
-// Check if the object was already initialized
-static inline bool
-is_object_initialized (struct object *ob)
-{
-  // We have to use acquire atomics for the read, which
-  // is a bit involved as we read from a bitfield
-  struct object tmp;
-  __atomic_load (&(ob->s.b), &(tmp.s.b), __ATOMIC_ACQUIRE);
-  return tmp.s.b.sorted;
-}
-
-#endif
-
 const fde *
 _Unwind_Find_FDE (void *pc, struct dwarf_eh_bases *bases)
 {
@@ -1199,27 +1032,17 @@ struct object *ob;
   const fde *f = NULL;
 
 #ifdef ATOMIC_FDE_FAST_PATH
-  ob = btree_lookup (&registered_frames, (uintptr_type) pc);
-  if (!ob)
+  /* For targets where unwind info is usually not registered through these
+     APIs anymore, avoid taking a global lock.
+     Use relaxed MO here, it is up to the app to ensure that the library
+     loading/initialization happens-before using that library in other
+     threads (in particular unwinding with that library's functions
+     appearing in the backtraces).  Calling that library's functions
+     without waiting for the library to initialize would be racy.  */
+  if (__builtin_expect (!__atomic_load_n (&any_objects_registered,
+					  __ATOMIC_RELAXED), 1))
     return NULL;
-
-  // Initialize the object lazily
-  if (!is_object_initialized (ob))
-    {
-      // Check again under mutex
-      init_object_mutex_once ();
-      __gthread_mutex_lock (&object_mutex);
-
-      if (!ob->s.b.sorted)
-	{
-	  init_object (ob);
-	}
-
-      __gthread_mutex_unlock (&object_mutex);
-    }
-
-  f = search_object (ob, pc);
-#else
+#endif
 
   init_object_mutex_once ();
   __gthread_mutex_lock (&object_mutex);
@@ -1257,7 +1080,6 @@ }
 
  fini:
   __gthread_mutex_unlock (&object_mutex);
-#endif
 
   if (f)
     {
--- gcc-13.4.0/libgcc/unwind-dw2-fde.h
+++ gcc-13.4.0/libgcc/unwind-dw2-fde.h
@@ -166,7 +166,7 @@ }
 extern const fde * _Unwind_Find_FDE (void *, struct dwarf_eh_bases *);
 
 static inline int
-last_fde (const struct object *obj __attribute__ ((__unused__)), const fde *f)
+last_fde (struct object *obj __attribute__ ((__unused__)), const fde *f)
 {
 #ifdef DWARF2_OBJECT_END_PTR_EXTENSION
   return f == (const fde *) obj->fde_end || f->length == 0;
